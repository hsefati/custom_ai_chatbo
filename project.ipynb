{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cb5ff",
   "metadata": {},
   "source": [
    "For implementation of a chatbot with the help foundation modeol like 'GPT-3.5-turbo-instruct', a database needed to able us to implement RAG.\n",
    "To this end, \"Character Description\" database is selected for the following:\n",
    "* Clear separation of data category\n",
    "* Clean Format\n",
    "* Contextually appropirate to create context for a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5f570",
   "metadata": {},
   "source": [
    "# Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "267d1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set opanai api key\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "openai.api_key = \"XXXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb3a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file from local path\n",
    "df = pd.read_csv(\"./data/character_descriptions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5efcb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'text' column by combining all relevant columns\n",
    "df['text'] = df.apply(lambda row: f\"{row['Name']}: {row['Description']} This is a {row['Medium']} set in {row['Setting']}.\", axis=1)\n",
    "\n",
    "# Keep only the 'text' column\n",
    "df_transformed = df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1e66ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed data to a new csv file\n",
    "df_transformed.to_csv(\"./data/character_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "## Custom Query Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30549fd5",
   "metadata": {},
   "source": [
    "### Creating an Embeddings Index with `openai.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b6e1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_costum_embedding(text):\n",
    "    response = openai.Embedding.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=text\n",
    "    )\n",
    "    return response[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13f2dcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26777/2047624594.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_transformed[\"embeddings\"] = df_transformed[\"text\"].apply(get_costum_embedding)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each character description\n",
    "df_transformed[\"embeddings\"] = df_transformed[\"text\"].apply(get_costum_embedding)\n",
    "# Save embeddings for future use\n",
    "df_transformed.to_csv(\"./data/character_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385acd47",
   "metadata": {},
   "source": [
    "### Finding Relevant Data with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c403f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_sorted_by_relevance(prompt, df):   \n",
    "\t# Get embeddings for the prompt text\n",
    "\tprompt_embeddings = get_embedding(prompt, engine=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "\tdf_copy = df.copy()\n",
    "\tdf_copy[\"distances\"] = distances_from_embeddings(\n",
    "\t\t\tprompt_embeddings,\n",
    "\t\t\tdf_copy[\"embeddings\"].values,\n",
    "\t\t\tdistance_metric=\"cosine\"\n",
    "\t)\n",
    "\n",
    "\t# Sort the copied dataframe by the distances and return it\n",
    "\t# (shorter distance = more relevant so we sort in ascending order)\n",
    "\tdf_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "\treturn df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74280b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from the csv file\n",
    "df_embedding = pd.read_csv(\"./data/character_embeddings.csv\")\n",
    "# Make sure the embeddings are in the correct format\n",
    "df_embedding[\"embeddings\"] = df_embedding[\"embeddings\"].apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc768d",
   "metadata": {},
   "source": [
    "### Tokenizing with `tiktoken` and compsoing a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea03825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question\n",
    "can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "\n",
    "    context = []\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "\n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "\n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "## Custom Performance Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6d0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=2000, max_answer_tokens=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "\n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = create_prompt(question, df, max_prompt_tokens)\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=COMPLETION_MODEL_NAME,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11fdc0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4901c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_answer = answer_question(\"who old is jack from england?\", df_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7a093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40s'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f646989",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_answer = answer_question(\"Give me an acter from USA with Muscial medium?\", df_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11c07a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Donna, Johnny, Dolly, Crystal, Karma, Sable, Olivia'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689966a",
   "metadata": {},
   "source": [
    "### Chatbot Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "985c6b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Jack is described as a middle-aged man in his 40s, a successful businessman, and Sarah's boss. He has a no-nonsense attitude and is fiercely loyal to his friends and family. He is married to Alice and the play is set in England.\n",
      "Chatbot: Donna could be considered an actor from USA with Musical medium as she is a seasoned performer on stage in a Musical set in USA.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: Goodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(question, df, max_prompt_tokens, max_answer_tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manswer_question\u001b[39m(\n\u001b[1;32m      2\u001b[0m     question, df, max_prompt_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, max_answer_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      3\u001b[0m ):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Given a question, a dataframe containing rows of text, and a maximum\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    number of desired tokens in the prompt and response, return the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    If the model produces an error, return an empty string\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_prompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     16\u001b[0m             model\u001b[38;5;241m=\u001b[39mCOMPLETION_MODEL_NAME,\n\u001b[1;32m     17\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     18\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39mmax_answer_tokens\n\u001b[1;32m     19\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mcreate_prompt\u001b[0;34m(question, df, max_token_count)\u001b[0m\n\u001b[1;32m     23\u001b[0m current_token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt_template)) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     24\u001b[0m                         \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(question))\n\u001b[1;32m     26\u001b[0m context \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_rows_sorted_by_relevance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Increase the counter based on the number of tokens in this row\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     text_token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n\u001b[1;32m     31\u001b[0m     current_token_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text_token_count\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mget_rows_sorted_by_relevance\u001b[0;34m(prompt, df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_rows_sorted_by_relevance\u001b[39m(prompt, df):   \n\u001b[1;32m      2\u001b[0m \t\u001b[38;5;66;03m# Get embeddings for the prompt text\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \tprompt_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDING_MODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \tdf_copy \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      6\u001b[0m \tdf_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m distances_from_embeddings(\n\u001b[1;32m      7\u001b[0m \t\t\tprompt_embeddings,\n\u001b[1;32m      8\u001b[0m \t\t\tdf_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      9\u001b[0m \t\t\tdistance_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \t)\n",
      "File \u001b[0;32m~/workspace/gen_ai/.venv/lib/python3.9/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/gen_ai/.venv/lib/python3.9/site-packages/tenacity/__init__.py:485\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    484\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/workspace/gen_ai/.venv/lib/python3.9/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = answer_question(user_input, df_embedding)\n",
    "    print(\"Chatbot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
